{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redditwarp.SYNC\n",
    "import feedparser\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "from pprint import pp\n",
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "from rcd.config.log_config import setup_logging\n",
    "from rcd.config import get_module_config, settings\n",
    "\n",
    "log = logging.getLogger(\"rec_news\")\n",
    "config = get_module_config(\"news\")\n",
    "\n",
    "timestamps_file = \"timestamps.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_timestamp(channel):\n",
    "    try:\n",
    "        with open(timestamps_file, \"r\") as f:\n",
    "            timestamps = json.load(f)\n",
    "        return timestamps.get(channel, None)\n",
    "    except (FileNotFoundError, ValueError, json.JSONDecodeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def set_latest_timestamp(channel, timestamp):\n",
    "    try:\n",
    "        with open(timestamps_file, \"r\") as f:\n",
    "            timestamps = json.load(f)\n",
    "    except (FileNotFoundError, ValueError, json.JSONDecodeError):\n",
    "        timestamps = {}\n",
    "\n",
    "    timestamps[channel] = timestamp\n",
    "\n",
    "    with open(timestamps_file, \"w\") as f:\n",
    "        json.dump(timestamps, f)\n",
    "\n",
    "\n",
    "def fetch_reddit_comments(client, subreddit):\n",
    "    \"\"\"\n",
    "    Retrieve comments newer than timestamp from the daily.\n",
    "    \"\"\"\n",
    "    # The daily should be within top 5 hottest submissions\n",
    "    hot = client.p.subreddit.pull.hot(subreddit, amount=5)\n",
    "    comments = []\n",
    "    count = 0\n",
    "    newest_comment_ts = get_latest_timestamp(subreddit)\n",
    "    for s in hot:\n",
    "        if \"daily\" in s.title.lower():\n",
    "            count = s.comment_count\n",
    "            tree_node = client.p.comment_tree.fetch(s.id36, sort=\"new\", limit=50)\n",
    "            for ch in tree_node.children:\n",
    "                c = ch.value\n",
    "                if newest_comment_ts and c.created_ut <= newest_comment_ts:\n",
    "                    continue\n",
    "                comment = {\n",
    "                    \"id\": f\"{c.submission.id36}+{c.id36}\",\n",
    "                    \"ts\": c.created_ut,\n",
    "                    \"author\": c.author_display_name,\n",
    "                    \"body\": c.body,\n",
    "                    \"subredit\": subreddit,\n",
    "                }\n",
    "                comments.append(comment)\n",
    "\n",
    "    if len(comments) > 0:\n",
    "        new_latest_ts = max(d[\"ts\"] for d in comments if \"ts\" in d)\n",
    "        set_latest_timestamp(subreddit, new_latest_ts)\n",
    "\n",
    "    return comments, count\n",
    "\n",
    "\n",
    "def load_feed_urls():\n",
    "    feed_csvs = {\n",
    "        \"crypto\": \"../config/crypto_feeds.csv\",\n",
    "        \"tradfi\": \"../config/tradfi_feeds.csv\",\n",
    "    }\n",
    "\n",
    "    feeds = {\n",
    "        \"crypto\": [],\n",
    "        \"tradfi\": [],\n",
    "    }\n",
    "\n",
    "    for feed_name, csv_file in feed_csvs.items():\n",
    "        with open(csv_file) as csvfile:\n",
    "            csv_reader = csv.reader(csvfile)\n",
    "            for row in csv_reader:\n",
    "                feeds[feed_name].append({\"source\": row[0], \"url\": row[1]})\n",
    "\n",
    "    return feeds\n",
    "\n",
    "\n",
    "def get_article_timestamp(entry):\n",
    "    try:\n",
    "        if hasattr(entry, \"published_parsed\") and entry.published_parsed:\n",
    "            return time.mktime(entry.published_parsed)\n",
    "        else:\n",
    "            date_format = \"%b %d, %Y %H:%M %Z\"\n",
    "            date_obj = datetime.strptime(entry.published, date_format).replace(\n",
    "                tzinfo=pytz.UTC\n",
    "            )\n",
    "            return date_obj.timestamp()\n",
    "    except Exception as e:\n",
    "        log.warning(f\"Error parsing timestamp: {entry.published_parsed}\")\n",
    "        log.exception(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_cointelegraph_feed():\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    feed_url = \"https://cointelegraph.com/rss\"\n",
    "    response = requests.get(feed_url, headers=headers)\n",
    "    feed_content = response.content.decode(\"utf-8\")\n",
    "    return feed_content\n",
    "\n",
    "\n",
    "def get_content(rss_content):\n",
    "    for content in rss_content:\n",
    "        if \"type\" in content and content[\"type\"] == \"text/plan\":\n",
    "            return content.value\n",
    "\n",
    "    c = \" \".join(content_dict.value for content_dict in rss_content)\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "def fetch_news(feed_list):\n",
    "    news = []\n",
    "    for item in feed_list:\n",
    "        # skip invalid URLs\n",
    "        if not \"http\" in item[\"url\"]:\n",
    "            continue\n",
    "\n",
    "        latest_ts = get_latest_timestamp(item[\"source\"])\n",
    "        if latest_ts:\n",
    "            log.info(\n",
    "                f\"Fetching news from: {item['source']} newer than {datetime.fromtimestamp(latest_ts)}\"\n",
    "            )\n",
    "        else:\n",
    "            log.info(f\"Fetching all news from {item['source']}\")\n",
    "\n",
    "        # cointelegraph clouflare workaround\n",
    "        if \"cointelegraph\" in item[\"url\"]:\n",
    "            item[\"url\"] = fetch_cointelegraph_feed()\n",
    "\n",
    "        feed = feedparser.parse(item[\"url\"])\n",
    "        feed_entries = []\n",
    "        for entry in feed.entries:\n",
    "            ts = get_article_timestamp(entry)\n",
    "            if not ts:\n",
    "                log.warning(f\"Skipping entry from {item['source']} - no timestamp\")\n",
    "                continue\n",
    "            if latest_ts and ts <= latest_ts:\n",
    "                continue\n",
    "            link = entry.link\n",
    "            title = entry.title if \"title\" in entry else \"\"\n",
    "            summary = entry.summary if \"summary\" in entry else \"\"\n",
    "            published = entry.published if \"published\" in entry else \"\"\n",
    "\n",
    "            if summary:\n",
    "                soup = BeautifulSoup(summary, \"html.parser\")\n",
    "                summary = soup.get_text()\n",
    "\n",
    "            # The content attribute is a list of content dictionaries\n",
    "            content = \"\"\n",
    "            if \"content\" in entry:\n",
    "                content = get_content(entry.content)\n",
    "\n",
    "            news_entry = {\n",
    "                \"source\": item[\"source\"],\n",
    "                \"link\": link,\n",
    "                \"title\": title,\n",
    "                \"summary\": summary,\n",
    "                \"content\": content,\n",
    "                \"published\": published,\n",
    "                \"ts\": ts,\n",
    "            }\n",
    "            feed_entries.append(news_entry)\n",
    "\n",
    "        news.extend(feed_entries)\n",
    "        if len(feed_entries) > 0:\n",
    "            new_latest_ts = max(d[\"ts\"] for d in feed_entries if \"ts\" in d)\n",
    "            set_latest_timestamp(item[\"source\"], new_latest_ts)\n",
    "\n",
    "    return news\n",
    "\n",
    "\n",
    "def save_to_db(db, channel, data):\n",
    "    # Insert documents into the collection\n",
    "    collection = db[channel]\n",
    "    result = collection.insert_many(data)\n",
    "    log.info(f\"Inserted {len{result.inserted_ids}} into {channel}\")\n",
    "\n",
    "    # Print the IDs of the inserted documents\n",
    "    print(\"Inserted IDs:\", result.inserted_ids)\n",
    "\n",
    "\n",
    "def save_to_json(channel, data):\n",
    "    filename = f\"{channel}.json\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l3/s2pn_md103b8s6_896qfc3v80000gn/T/ipykernel_79040/1414912762.py:109: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(c, \"html.parser\")\n",
      "/var/folders/l3/s2pn_md103b8s6_896qfc3v80000gn/T/ipykernel_79040/1414912762.py:147: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(summary, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "db_config = settings[config.db_name]\n",
    "\n",
    "reddit_client = redditwarp.SYNC.Client()\n",
    "db_client = MongoClient(db_config.mongo.conn_str)\n",
    "feeds = load_feed_urls()\n",
    "news = {\"crypto\": [], \"tradfi\": []}\n",
    "subreddits = config.reddit.subreddits\n",
    "reddit_comments = []\n",
    "\n",
    "for feed_name, feed_list in feeds.items():\n",
    "    news[feed_name] = fetch_news(feed_list)\n",
    "\n",
    "for sr in subreddits:\n",
    "    reddit_comments.extend(fetch_reddit_comments(reddit_client, sr))\n",
    "\n",
    "for channel, entries in news.items():\n",
    "    save_to_json(f\"news_{channel}\", entries)\n",
    "\n",
    "save_to_json(\"reddit\", reddit_comments)\n",
    "# save_to_db(db_client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
